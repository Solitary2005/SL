{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding an Input Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Life is short, eat dessert first'\n",
    "dc = {s:i for i,s \n",
    "      in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sentence_int = torch.tensor(\n",
    "    [dc[s] for s in sentence.replace(',', '').split()]\n",
    ")\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778, -0.3035],\n",
      "        [ 0.1794,  1.8951,  0.4954],\n",
      "        [ 0.2692, -0.0770, -1.0205],\n",
      "        [-0.2196, -0.3792,  0.7671],\n",
      "        [-0.5880,  0.3486,  0.6603],\n",
      "        [-1.1925,  0.6984, -1.4097]])\n",
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50_000\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 使用一个微小的3维嵌入，使每个输入单词由一个3维向量表示\n",
    "embed = torch.nn.Embedding(vocab_size, 3)\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "\n",
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，嵌入大小通常从数百到数千个维度不等。例如，Llama 2 使用的嵌入大小为 4,096。我们在这里使用 3 维嵌入的原因纯粹是为了说明目的\n",
    "由于句子由 6 个单词组成，这将导致 6×3 维嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Weight Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与前面的单词 embedding vectors 类似，维度 d_q、d_k、d_v 通常要大得多，但我们在这里使用小数字进行说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![本地路径](picture01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，q（i） 和 k（i） 都是维度 dk 的向量。投影矩阵 Wq 和 Wk 的形状为 d × dk ，而 Wv 的形状为 d × dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "\n",
    "d_q, d_k, d_v = 2, 2, 4\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d, d_q))\n",
    "W_key = torch.nn.Parameter(torch.rand(d, d_k))\n",
    "W_value = torch.nn.Parameter(torch.rand(d, d_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1]\n",
    "# 将输入张量 x_2 分别与三个不同的权重矩阵（W_query、W_key 和 W_value）进行矩阵乘法\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "keys = embedded_sentence @ W_key\n",
    "values = embedded_sentence @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![网页形式](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示，我们计算 ωi，j 作为查询序列和键序列之间的点积，ωi，j =q（i）k（j）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，我们可以计算查询和第 5 个输入元素（对应于索引位置 4）的非规范化注意力权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2903, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "omega_24 = query_2.dot(keys[4])\n",
    "print(omega_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们计算所有输入标记的 ω 值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6004,  3.4707, -1.5023,  0.4991,  1.2903, -1.3374],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "omega_2 = query_2 @ keys.T\n",
    "print(omega_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![网页形式](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# 归一化注意力权重\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![网页形式](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([0.5313, 1.3607, 0.7891, 1.3110], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2 @ values\n",
    "\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的Self-Attention\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T  # 未归一化的注意力权重    \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1564,  0.1028, -0.0763, -0.0764],\n",
      "        [ 0.5313,  1.3607,  0.7891,  1.3110],\n",
      "        [-0.3542, -0.1234, -0.2626, -0.3706],\n",
      "        [ 0.0071,  0.3345,  0.0969,  0.1998],\n",
      "        [ 0.1008,  0.4780,  0.2021,  0.3674],\n",
      "        [-0.5296, -0.2799, -0.4107, -0.6006]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 一个简单的例子\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 因为我们有 4 个头，所以将 d_out_v 从 4 降到 1。\n",
    "d_in, d_out_kq, d_out_v = 3, 2, 4\n",
    "\n",
    "sa = SelfAttention(d_in, d_out_kq, d_out_v)\n",
    "print(sa(embedded_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiHeadAttention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [SelfAttention(d_in, d_out_kq, d_out_v) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0185,  0.0170,  0.1999, -0.0860],\n",
      "        [ 0.4003,  1.7137,  1.3981,  1.0497],\n",
      "        [-0.1103, -0.1609,  0.0079, -0.2416],\n",
      "        [ 0.0668,  0.3534,  0.2322,  0.1008],\n",
      "        [ 0.1180,  0.6949,  0.3157,  0.2807],\n",
      "        [-0.1827, -0.2060, -0.2393, -0.3167]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d_in, d_out_kq, d_out_v = 3, 2, 1\n",
    "\n",
    "block_size = embedded_sentence.shape[1]\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out_kq, d_out_v, num_heads=4\n",
    ")\n",
    "\n",
    "context_vecs = mha(embedded_sentence)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下：\n",
    "![网页形式](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb75a8df1-0a82-4f79-8e68-4fe16587063d_1474x1108.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![网页形式](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa16d2bd0-984d-4224-9c4c-2f0cc144a599_1520x1108.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x_1, x_2):           # x_2 is new\n",
    "        queries_1 = x_1 @ self.W_query\n",
    "        \n",
    "        keys_2 = x_2 @ self.W_key          # new\n",
    "        values_2 = x_2 @ self.W_value      # new\n",
    "        \n",
    "        attn_scores = queries_1 @ keys_2.T # new \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
    "        \n",
    "        context_vec = attn_weights @ values_2\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input shape: torch.Size([6, 3])\n",
      "Second input shape: torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d_in, d_out_kq, d_out_v = 3, 2, 4\n",
    "\n",
    "crossattn = CrossAttention(d_in, d_out_kq, d_out_v)\n",
    "\n",
    "first_input = embedded_sentence\n",
    "second_input = torch.rand(8, d_in)\n",
    "\n",
    "print(\"First input shape:\", first_input.shape)\n",
    "print(\"Second input shape:\", second_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4231, 0.8665, 0.6503, 1.0042],\n",
      "        [0.4874, 0.9718, 0.7359, 1.1353],\n",
      "        [0.4054, 0.8359, 0.6258, 0.9667],\n",
      "        [0.4357, 0.8886, 0.6678, 1.0311],\n",
      "        [0.4429, 0.9006, 0.6775, 1.0460],\n",
      "        [0.3860, 0.8021, 0.5985, 0.9250]], grad_fn=<MmBackward0>)\n",
      "Output shape: torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "context_vectors = crossattn(first_input, second_input)\n",
    "\n",
    "print(context_vectors)\n",
    "print(\"Output shape:\", context_vectors.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
